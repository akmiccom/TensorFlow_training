{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 深層畳み込み敵対的生成ネットワーク(DCGAN)\n",
    "\n",
    "- GAN とは？\n",
    "  - 敵対的生成ネットワーク (GAN) は現在コンピュータサイエンス分野で最も興味深い構想です\n",
    "  - 2 つのモデルが敵対的なプロセスにより同時にトレーニングされます\n",
    "  - ジェネレータ(芸術家)が本物のような画像の制作を学習する一方で、ディスクリミネータ(芸術評論家)は本物の画像を偽物と見分けることを学習します\n",
    "- トレーニングにより\n",
    "  - ジェネレータでは、本物に見える画像の作成が徐々に上達する\n",
    "  - ディスクリミネータでは、本物と偽物の区別が上達します\n",
    "  - ディスクリミネータが本物と偽物の画像を区別できなくなった時点で平衡に達します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.17.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "import glob\n",
    "import imageio\n",
    "# import imageio.v2 as imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (_, _) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\n",
    "train_images = (train_images - 127.5) / 127.5  # Normalize the images to [-1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 60000\n",
    "BATCH_SIZE = 256\n",
    "# Batch and shuffle the data\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ジェネレータ\n",
    "- Conv2DTranspose (アップサンプリング) レイヤーを使用して、シード (ランダムノイズ) から画像を生成\n",
    "- このシードを入力として取る Dense レイヤーから始め\n",
    "- 待する画像サイズ (28x28x1) に到達するまで何度もアップサンプリングします\n",
    "- tanh を使用する出力レイヤーを除き、各レイヤーに LeakyReLU アクティベーションが使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator_model():\n",
    "    model = tf.keras.Sequential(name='generator')\n",
    "    model.add(tf.keras.Input(shape=(100,), name='input'))\n",
    "\n",
    "    model.add(layers.Dense(7*7*256, use_bias=False))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Reshape((7, 7, 256)))\n",
    "    assert model.output_shape == (None, 7, 7, 256)\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 7, 7, 128)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 14, 14, 64)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 28, 28, 1)\n",
    "\n",
    "    return model\n",
    "\n",
    "model = make_generator_model()\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ジェネレータテスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 100)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generator = make_generator_model()\n",
    "noise = tf.random.normal(shape=[1, 100]) # N(0, 1)を100出力\n",
    "print(noise.shape)\n",
    "# noise = tf.random.Generator.from_seed(1).normal(shape=[1, 100])\n",
    "generated_images = generator(noise, training=False)\n",
    "\n",
    "plt.imshow(generated_images[0, :, :, 0], cmap='gray')\n",
    "plt.axis(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ディスクリミネータ\n",
    "- CNNベースの画像分類子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_discriminator_model():\n",
    "    model = tf.keras.Sequential(name='discriminator')\n",
    "    model.add(tf.keras.Input(shape=(28, 28, 1), name='input'))\n",
    "    \n",
    "    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = make_discriminator_model()\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ディスクリミネータ出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.00034261]], dtype=float32)>"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discriminator = make_discriminator_model()\n",
    "dicision = discriminator(generated_images)\n",
    "dicision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ディスクリミネータの損失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ジェネレータの損失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "オプティマイザー"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_oprimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_oprimizer = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "チェックポイントの保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = \"../notebook/training_checkpoint\"\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(\n",
    "    generator_oprimizer=generator_oprimizer,\n",
    "    discriminator_oprimizer=discriminator_oprimizer,\n",
    "    generator=generator,\n",
    "    discriminator=discriminator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "トレーニンググループの定義\n",
    "- ランダムシードを入力として受け取っているジェネレータから始まります\n",
    "- そのシードを使って画像が生成されると、\n",
    "  - ディスクリミネータを使って本物の画像（トレーニングセットから取り出された画像）と偽物の画像（ジェネレータが生成した画像）が分類されます\n",
    "  - これらの各モデルに対して損失が計算されると、勾配を使用してジェネレータとディスクリミネータが更新されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "noise_dim = 100\n",
    "num_examples_to_generate = 16\n",
    "\n",
    "seed = tf.random.normal([num_examples_to_generate, noise_dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images):\n",
    "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(noise, training=True)\n",
    "\n",
    "        real_output = discriminator(images, training=True)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_oprimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_oprimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "画像を生成して保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, test_input):\n",
    "    predictions = model(test_input, training=False)\n",
    "\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "\n",
    "    for i in range(predictions.shape[0]):\n",
    "        plt.subplot(4, 4, i+1)\n",
    "        plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "        plt.axis(False)\n",
    "\n",
    "    plt.savefig(f'../images/imega_at_epoch{epoch:04d}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "トレーニング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "\n",
    "        for image_batch in dataset:\n",
    "            train_step(image_batch)\n",
    "\n",
    "        # display.clear_output(wait=True)\n",
    "        generate_and_save_images(generator, epoch+1, seed)\n",
    "\n",
    "        if (epoch+1) % 15 == 0:\n",
    "            checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "\n",
    "        print(f'Time for epoch {epoch+1} is {time.time()-start}')\n",
    "\n",
    "    # display.clear_output(wait=True)\n",
    "    generate_and_save_images(generator, epochs, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yes_no_input():\n",
    "    while True:\n",
    "        choice = input(\"Please respond with 'yes' or 'no' [y/N]: \").lower()\n",
    "        if choice in ['y', 'ye', 'yes']:\n",
    "            return True\n",
    "        elif choice in ['n', 'no']:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STOP!\n"
     ]
    }
   ],
   "source": [
    "if yes_no_input():\n",
    "    print('OK!')\n",
    "    train(train_dataset, EPOCHS)\n",
    "else:\n",
    "    print('STOP!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(\"../images/imega*\")\n",
    "# print(len(files))\n",
    "\n",
    "images = []\n",
    "for file in files:\n",
    "    im = PIL.Image.open(file)\n",
    "    images.append(im)\n",
    "\n",
    "images[0].save(\n",
    "    \"../images/test.gif\",\n",
    "    save_all=True,\n",
    "    append_images=images[1:],\n",
    "    optimize=False,\n",
    "    duration=250,\n",
    "    loop=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_docs.vis.embed as embed\n",
    "# embed.embed_file(\"../images/test.gif\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
